# Introduction

It becomes popular that machine learning using libraries such as TensorFlow \cite{TensorFlow} and Keras \cite{Keras} is applied to social implementation. Thus, GPGPU technologies \cite{Nickolls:2008:SPP:1401132.1401152} is researched and developed because such machine learning requires more performance. However, current GPGPU libraries such as CUDA \cite{CUDA}, CuPy \cite{CuPy}, TensorFlow and Keras are two problems:

1. **Performance Problem:** Python \cite{Python} has basicly poor performance because it is based on single-threaded interpretation, except from native libraries such as NumPy \cite{NumPy} and CuPy \cite{CuPy}. This is serious when the system manipurates a mass of data by the Python interpreter before processing by the native libraries.
2. **Setting Problem:** The native libraries in Python using GPUs are difficult to set up. It requires troublesome installation and settings. Furthormore, they support only NVIDIA GPUs.

Thus, we adopt Elixir \cite{Elixir} for the purpose of machine learning, because it has awesome parallel programming performance \cite{Elixir16}. However, now Elixir has no GPGPU support. Thus, we try to implement GPGPU support in Elixir.

The rest of this paper is organized as follows: Section 2 proposes our core idea to convert an Elixir code to drive GPUs. Section 3 describes an implementation of our approach. Section 4 shows environments of our experiments, description of benchmarks and results and discussion. Section 5 concludes this paper and describes future works. 

# Our Idea

Our idea is shown in our previous work \cite{ZACKY18}: The parallel programming style using Flow \cite{Flow} based on the MapReduce model \cite{Dean:2008:MSD:1327452.1327492}, e.g. shown in **Fig.** \ref{fig:flow}, fits the SIMD (Single Instruction Multiple Data) architecture that is adopted as GPUs, e.g. pipelined functions `foo |> bar` and a list generated by the range `1..1000` correspond 'single instruction' and 'multiple data', respectively. Thus, an Elixir code using Flow can be converted to a GPU executable code, easily.

\begin{figure}[t]
\setbox0\vbox{
\begin{verbatim}
1..1000
  |> Flow.from_enumerable()
  |> Flow.map(foo)
  |> Flow.map(bar)
  |> Enum.to_list 
\end{verbatim}
}
\centerline{\fbox{\box0}}
\caption{Elixir code of list manipulation using Flow.}
\label{fig:flow}
\end{figure}


# Implementation

We adopt the Logistic Maps over prime fields \cite{Miyazaki14}, whose recurrence relation is shown in the following equation as a benchmark because it requires much integer calculation, which can be converted to a GPU assembly code easily:

\begin{displaymath}
  X_{i+1} = \mu_p X_i (X_i + 1) \mod p
\end{displaymath}

We have implemented it in our previous work \cite{emb48, ZACKY18}. Then, we describe the native code in our benchmark in Rust \cite{Rust} using Rustler \cite{Rustler} and ocl \cite{ocl}. This leads to awesome easiness of setting: it requires only installing languages, OpenCL \cite{OpenCL} and our benchmark by each one-sentence command. This is a big advantage over Python\cite{Python}, CUDA\cite{CUDA} and related libraries such as CuPy\cite{CuPy}.  

Our implementation is published in GitHub \footnote{LogisticMap: Benchmark of Logistic Map using integer calculation and Flow, available at https://github.com/zeam-vm/logistic\_map}.

# Performance Evaluation

## Evaluation Environment


We have evaluated two environments: Mac Pro (Mid 2010) and Google Compute Engine(GCE) \cite{GCE}. 

1. Mac Pro (Mid 2010) has one 2.8GHz Quad-Core Intel Xeon with 16GB memories and ATI Radeon HD 5770 with 1024MB memories. We use the following versions of software:
	* macOS: Sierra 10.12.6
	* Elixir \cite{Elixir}: 1.6.1 (OTP 20)
	* Rust \cite{Rust}: 1.26.0
	* OpenCL \cite{OpenCL}: 1.2
	* Python \cite{Python}: 3.6.0 (Anaconda 4.3.0)
	* Rustler \cite{Rustler}: 0.16.0
	* ocl \cite{ocl}: 0.18
	* CUDA \cite{CUDA} : N/A
	* NumPy \cite{NumPy}: 1.11.3
	* CuPy \cite{CuPy}: N/A

2. Our GCE settings include as follows:
	\begin{itemize}
	\item Machine type: custom (8 vCPUs and 16GB memories)
	\item CPU platform: Intel Broadwell
	\item GPU: one NVIDIA Tesla K80
	\item Zone: us-west1-b
	\end{itemize}
   We use the following versions of software:
	* ubuntu: 16.04
	* Elixir \cite{Elixir}: 1.6.5 (OTP 20.3)
	* Rust \cite{Rust}: 1.26.1
	* OpenCL \cite{OpenCL}: 1.2
	* Python \cite{Python}: 3.5.2
	* Rustler \cite{Rustler}: 0.16.0
	* ocl \cite{ocl}: 0.16
	* CUDA \cite{CUDA} : 9.0 (in case of using CuPy), 9.2 (other)
	* NumPy \cite{NumPy}: 1.14.3
	* CuPy \cite{CuPy}: 4.1.0


## Benchmarks

We will show the following benchmarks:

* Elixir\_recursive: that is written in only Elixir \cite{Elixir}. The calculation of the logistic maps is implemented using 10 times recursive calls.
* Elixir\_inlining: that is written in only Elixir. The calculation of the logistic maps is inlined inside of Flow \cite{Flow}. 
* Elixir\_Rustler\_CPU: that is written in Elixir and Rust with Rustler \cite{Rustler}. The logistic maps are calculated by CPU with the native code in Rust \cite{Rust}.
* Elixir\_Rustler\_GPU: that is written in Elixir and Rust with Rustler. The logistic maps are calculated by GPU via OpenCL \cite{OpenCL} with the native code in Rust and ocl \cite{ocl}.
* Empty: that is a dummy benchmark to be compared from a efficient point of view. It includes conversions between expressions of a list in Elixir and a vector in Rust, though it does not include the caliculation.
* Rust\_CPU: that is written in only Rust. The logistic maps are calculated by CPU.
* Rust\_GPU: that is written in only Rust. The logistic maps are calculated by GPU via OpenCL with ocl.
* Python\_CPU: that is written in only \cite{Python} with NumPy \cite{NumPy}.
* Python\_GPU: that is written in only \cite{Python} with CuPy \cite{CuPy} with GPU.

## Evaluation Result and Discussion

Table \ref{result} shows the result of the benchmarks. 

* Elixir\_Rustler\_GPU is 1.76--2.12 times faster than pure Elixir (Elixir\_recursive and Elixir\_inlining).
* The ratio of the difference between Elixir\_Rustler\_GPU and Empty is 8.29--10.5 percents. We indentify that it is net execution time apart from the overhead of the conversions between expressions of a list in Elixir and a vector in Rust.
* Pure Elixir and Elixir\_Rustler\_GPU is 1.19--1.53 times and 2.52--2.7 times faster than Python\_CPU, respectively.
* Elixir\_Rustler\_GPU is almost as fast as Python\_GPU.
* The ratios of the difference between Bnechmark8 and Rust\_CPU and between Elixir\_Rustler\_GPU and Rust\_GPU is 72.0--78.2 and 71.7--82.3 percents, respectively. We identify that it is the overhead of Erlang VM. 
* Rust\_GPU is 3.54--5.66 times faster than Elixir\_Rustler\_GPU and Python\_GPU. This is the potential of optimization.

\begin{table*}[t]
\centering
\caption{The result of the benchmarks}
\label{result}
\begin{tabular}{lll|r|r|}
           &                  &              & \multicolumn{1}{l|}{Mac Pro (Mid 2010)} & \multicolumn{1}{l|}{GCE}              \\
           &                  &              & \multicolumn{1}{l|}{2.8GHz Quad-Core Intel Xeon} & \multicolumn{1}{l|}{Intel Broadwell vCPU:8}           \\
           &                  &              & \multicolumn{1}{l|}{ATI Radeon HD 5770} & \multicolumn{1}{l|}{NVIDIA Tesla K80} \\ 
           &                  &              &  (sec)        & (sec) \\ \hline
Elixir\_recursive & Elixir      & recursive call & 12.796           & 9.537            \\
Elixir\_inlining & Elixir      & inlining     & 11.571             & 8.020            \\
Elixir\_Rustler\_CPU & Elixir / Rustler & CPU          & 9.703              & 7.908            \\
\rowcolor[HTML]{C0C0C0} 
Elixir\_Rustler\_GPU & Elixir / Rustler & OpenCL (GPU) & 6.572              & 4.494            \\
Empty      & Elixir / Rustler & empty        & 6.027              & 4.022            \\
Rust\_CPU  & Rust        & CPU          & 2.721              & 1.723            \\
\rowcolor[HTML]{C0C0C0} 
Rust\_GPU  & Rust        & OpenCL (GPU) & 1.857              & 0.794           \\
Python\_CPU & Python      & NumPy (CPU)  & 17.749              & 11.341 \\
Python\_GPU & Python      & CuPy (GPU)   & N/A                & 4.316 \\           
\end{tabular}
\end{table*}

# Conclusion and Future Works

To solve the performance and setting problems of Python and its libraries, we have proposed to convert an Elixir code using Flow to a GPU executable code because such an Elixir code fits the SIMD architecture that is adopted as GPUs.

We have shown a demonstration benchmark using the Logistic Maps. We describe the native code in our benchmark in Rust using Rustler and ocl. This leads to awesome easiness of setting. Our implementation is published in GitHub \cite{logistic_map}.

We have conducted the performance evaluation of the experimental implementation of GPGPU by Elixir and Rustler. We have got the following results:

* Elixir and Rustler code using GPU is 1.76--2.12 times and 2.52--2.7 times faster than pure Elixir and Python code executed by only CPU, respectively.
* The performance gap of Elixir and Rustler code using GPU by our strategy is only 3.54--5.66 times compared with native code using GPU. 
* Additionally, our Hastega method for Elixir achieves as almost fast as Python code with GPU. 

We realize that Erlang VM is not enough performance for such optimization, including elimination of the conversions between lists and vectors, which is the main reason of the overhead. Thus, We will implement Hastega, which is a new processing system of Elixir, which has a enough ability to drive GPUs and optimize the conversions.

We also have a plan to implement mathematic and machine learning libraries in Elixir, which is implemented using Flow. Of course, we will apply Hastega to the libraries, and compare them with that of Python.
