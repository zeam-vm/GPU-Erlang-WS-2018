# Introduction

GPGPU technologies \cite{Nickolls:2008:SPP:1401132.1401152} has been researched and developed actively. It enables a system to compute very parallel because the arhitecture of GPUs of the main stream is based on the SIMD (Single Instruction, Multiple Data) model, which is suitable for hyper parallel computing.

One of reasons that research and development of GPGPU has been attracted pay attention is that it becomes popular that machine learning using libraries is applied to social implementation, and such machine learning requires more performance. The de facto standard programming language of machine learning is Python \cite{Python}. However, current GPGPU libraries such as CUDA \cite{CUDA}, CuPy \cite{CuPy}, TensorFlow \cite{TensorFlow} and Keras \cite{Keras}, which are based on Python, have following two problems:

1. **Performance Problem:** Python \cite{Python} has basically poor performance because it is based on single-threaded interpretation, except from native libraries such as NumPy \cite{NumPy} and CuPy \cite{CuPy}. This is serious when the system manipurates a mass of data by the Python interpreter before processing by the native libraries.
2. **Setting Problem:** It is difficult to set up native libraries for Python using GPUs. They require troublesome installation and settings. Furthormore, they support only NVIDIA GPUs.

Thus, we adopt Elixir \cite{Elixir} for the purpose of machine learning, because it has awesome parallel programming performance \cite{Elixir16}. However, now Elixir has no GPGPU support. Thus, we try to implement GPGPU support in Elixir.

We propose the Hastega method that is based on a hypothesis that a programming style using Flow \cite{Flow} of Elixir, which is similar to the MapReduce model \cite{Dean:2008:MSD:1327452.1327492}, can be converted to GPGPU code easily. We also implement a GPGPU code to manipulate GPUs in Elixir and Rust \cite{Rust} using Rustler \cite{Rustler}. 

The rest of this paper is organized as follows: Section 2 proposes our core idea to convert an Elixir code to drive GPUs, and an implementation of our Hastega approach. Section 3 shows steps of installation and settings processes. Section 4 shows environments of our experiments, description of benchmarks and results and discussion. Section 5 concludes this paper and describes future works. 

# Hastega Strategy and Implementation

Our strategy is shown in our previous work \cite{ZACKY18}: The parallel programming style using Flow \cite{Flow} based on the MapReduce model \cite{Dean:2008:MSD:1327452.1327492}, as shown in **Fig. \ref{fig:flow}**, fits the SIMD (Single Instruction Multiple Data) architecture that is adopted as GPUs, e.g. pipelined functions `foo |> bar` and a list generated by the range `1..1000` correspond 'single instruction' and 'multiple data', respectively. Thus, an Elixir code using Flow can be converted to a GPU executable code, easily.

\begin{figure}[t]
\setbox0\vbox{
{\small
\begin{verbatim}
1..1000
  |> Flow.from_enumerable()
  |> Flow.map(foo)
  |> Flow.map(bar)
  |> Enum.to_list 
\end{verbatim}
}
}
\centerline{\fbox{\box0}}
\caption{Elixir code of list manipulation using Flow.}
\label{fig:flow}
\end{figure}

**Fig. \ref{fig:Hastega-arch}** shows the Hastega architecture. Then, we implement the native code in Rust \cite{Rust} using Rustler \cite{Rustler} and ocl \cite{ocl}. This leads to awesome easiness of setting: it requires only installing languages, OpenCL \cite{OpenCL} and our Hastega by each one-sentence command. This is a big advantage over Python\cite{Python}, CUDA\cite{CUDA} and related libraries such as CuPy\cite{CuPy}.  

\begin{figure}[t]
\includegraphics{Hastega-arch}
\caption{The Hastega Architecture}
\label{fig:Hastega-arch}
\end{figure}


Our implementation is published in GitHub \footnote{LogisticMap: Benchmark of Logistic Map using integer calculation and Flow, available at https://github.com/zeam-vm/logistic\_map}.

# Easiness of Setting

**Table \ref{setting}** shows comparison of steps of setting processes in Google Compute Engine(GCE) \cite{GCE}. 

Installation of our Hastega is never tedious. Its building tool sets up itself, automatically. All you need is to install OpenCL \cite{OpenCL}, to install and set up Elixir \cite{Elixir} and Rust \cite{Rust}, and to install our Hastega.

Installing and setting CUDA \cite{CUDA} and OpenCL \cite{OpenCL} in CuPy \cite{CuPy} needs more steps than our Hastega because CuPy requires older CUDA \cite{CUDA}, which requires us to investigate Q \& A sites such as Stack Overflow \cite{StackOverflow}.

Disadvantage of Hastega is installation of runtime for programming languages: it requires two languages, Elixir \cite{Elixir} and Rust \cite{Rust}. 


\begin{table}[b]
\centering
\caption{Comparison of Steps of Installation and Setting Processes in GCE}
\label{setting}
{\small
\begin{tabular}{lrr}
                               & \multicolumn{1}{l}{CuPy} & \multicolumn{1}{l}{Hastega} \\ \hline
Installation of CUDA \& OpenCL & 4                        & 1                           \\
Installation of Languages      & 0                        & 4                           \\
Installation of Libraries      & 2                        & 1                           \\ \hline
\end{tabular}
}
\end{table}

# Performance Evaluation

## Evaluation Environment

We adopt the Logistic Maps over prime fields \cite{Miyazaki14}, whose recurrence relation is shown in the following equation as a benchmark because it requires much integer calculation, which can be converted to a GPU assembly code easily:

\begin{displaymath}
  X_{i+1} = \mu_p X_i (X_i + 1) \mod p
\end{displaymath}

We have implemented it in our previous work \cite{emb48, ZACKY18}. 

We have evaluated two environments: Mac Pro (Mid 2010) and GCE \cite{GCE}. 

1. Mac Pro (Mid 2010) has one 2.8GHz Quad-Core Intel Xeon with 16GB memories and ATI Radeon HD 5770 with 1024MB memories.

2. Our GCE settings include as follows:
	* Machine type: custom (8 vCPUs and 16GB memories)
	* CPU platform: Intel Broadwell
	* GPU: one NVIDIA Tesla K80
	* Zone: us-west1-b

**Table \ref{versions}** shows versions of software.

\begin{table*}[t]
\centering
\caption{Versions of Software}
\label{versions}
{\small
\begin{tabular}{l|ll}
                       & Mac Pro (Mid 2010)     & GCE                                      \\ \hline
OS                     & Sierra 10.12.6         & ubuntu 16.04                             \\
Elixir \cite{Elixir}   & 1.6.6 (OTP 21)         & 1.6.6 (OTP 21)                           \\
Flow \cite{Flow}       & 0.14                   & 0.14                                     \\
Rust \cite{Rust}       & 1.27.0                 & 1.27.0                                   \\
OpenCL \cite{OpenCL}   & 1.2                    & 1.2                                      \\
Rustler \cite{Rustler} & 0.17.1                 & 0.17.1                                   \\
ocl \cite{ocl}         & 0.18                   & 0.18                                     \\
rayon \cite{rayon}     & 1.0                    & 1.0                                      \\
scoped-pool \cite{scoped-pool} & 1.0.0          & 1.0.0                                    \\ \hline
Python \cite{Python}   & 3.6.0 (Anaconda 4.3.0) & 3.5.2                                    \\
CUDA \cite{CUDA}       & N/A                    & 9.0 (in case of using CuPy), 9.2 (other) \\
NumPy \cite{NumPy}     & 1.11.3                 & 1.14.3                                   \\
CuPy \cite{CuPy}       & N/A                    & 4.1.0
\end{tabular}
}
\end{table*}

## Benchmarks

We will show the following benchmarks:

* Elixir\_recursive: that is written in only Elixir \cite{Elixir}. The calculation of the logistic maps is implemented using 10 times recursive calls.
* Elixir\_inlining: that is written in only Elixir. The calculation of the logistic maps is inlined inside of Flow \cite{Flow}.
* Rustler\_CPU: that is written in Elixir and Rust with Rustler \cite{Rustler}. The logistic maps are calculated by CPU with the native code in Rust \cite{Rust}, asynchronous NIF calls using thread pool by scoped-pool \cite{scoped-pool} and single-threaded.
* Rustler\_CPU\_multi: that is similar to Rustler\_CPU, but the logistic maps are calculated with multi-threaded by rayon \cite{rayon} though a thread pool of rayon is disabled because of the restriction of our implementation.
* Rustler\_GPU: that is written in Elixir and Rust with Rustler. The logistic maps are calculated by GPU via OpenCL \cite{OpenCL} with the native code in Rust and ocl \cite{ocl}.
* Empty: that is a dummy benchmark to be compared from a efficient point of view. It includes conversions between expressions of a list in Elixir and a vector in Rust, though it does not include the caliculation.
* Rust\_CPU: that is written in only Rust. The logistic maps are calculated by CPU with single-threaded.
* Rust\_CPU\_multi: that is similar to Rust\_CPU, but the logistic maps are calculated with multi-threaded by rayon \cite{rayon} and a thread pool of rayon is enabled.
* Rust\_GPU: that is written in only Rust. The logistic maps are calculated by GPU via OpenCL with ocl.
* Python\_CPU: that is written in only \cite{Python} with NumPy \cite{NumPy}.
* Python\_GPU: that is written in only \cite{Python} with CuPy \cite{CuPy} with GPU.

## Evaluation Result and Discussion

**Table \ref{result}** shows the result of the benchmarks.

* Rustler\_GPU and Rustler\_CPU\_multi are 4.43--8.23 times and 5.68--6.97 times faster than pure Elixir, which means Elixir\_recursive and Elixir\_inlining.
* The order of Rustler\_CPU\_multi and Rustler\_GPU is converse in the cases of Mac Pro (Mid 2010) and GCE because the lack of the thread pool of rayon affects performance in the case of Linux.
* The ratio of the difference between Rustler\_GPU and Empty is 22.2--27.6 percents. We indentify that it is net execution time apart from the overhead of the conversions between expressions of a list in Elixir and a vector in Rust.
* Pure Elixir and Rustler\_GPU is 1.17--1.68 times and 7.43--9.64 times faster than Python\_CPU, respectively.
* Rustler\_GPU is 3.67 times faster than Python\_GPU.
* The ratios of the difference between Rustler\_CPU and Rust\_CPU and Rust\_CPU\_multi, and between Rustler\_GPU and Rust\_GPU is 62.0--70.0 and 32.2--35.2 percents, respectively. We identify that it is the overhead of Erlang VM.
* Rust\_GPU is 1.48--1.54 times faster than Rustler\_GPU. This is the potential of optimization.

\begin{table*}[t]
\centering
\caption{The result of the benchmarks}
\label{result}
{\small
\begin{tabular}{lll|r|r|}
           &                  &              & \multicolumn{1}{l|}{Mac Pro (Mid 2010)} & \multicolumn{1}{l|}{GCE}              \\
           &                  &              & \multicolumn{1}{l|}{2.8GHz Quad-Core Intel Xeon} & \multicolumn{1}{l|}{Intel Broadwell vCPU:8}           \\
           &                  &              & \multicolumn{1}{l|}{ATI Radeon HD 5770} & \multicolumn{1}{l|}{NVIDIA Tesla K80} \\ 
           &                  &              &  (sec)        & (sec) \\ \hline
Elixir\_recursive             & Elixir           & recursive call & 12.177           & 9.674            \\
Elixir\_inlining              & Elixir           & inlining       & 10.579           & 8.075            \\
Rustler\_CPU                  & Elixir / Rustler & CPU            & 7.691            & 6.098            \\
\rowcolor[HTML]{C0C0C0}
Rustler\_CPU\_multi           & Elixir / Rustler & CPU            & 1.748            & 1.422              \\
\rowcolor[HTML]{C0C0C0}
Rustler\_GPU                  & Elixir / Rustler & OpenCL (GPU)   & 2.388            & 1.176            \\
Empty                         & Elixir / Rustler & empty          & 1.859            & 0.852            \\
Rust\_CPU                     & Rust             & CPU            & 2.926            & 1.829            \\
\rowcolor[HTML]{C0C0C0}
Rust\_CPU\_multi              & Rust             & CPU            & 0.669            & 0.374             \\
\rowcolor[HTML]{C0C0C0}
Rust\_GPU                     & Rust             & OpenCL (GPU)   & 1.546            & 0.797           \\
Python\_CPU                   & Python           & NumPy (CPU)    & 17.749           & 11.341          \\
Python\_GPU                   & Python           & CuPy (GPU)     & N/A              & 4.316           \\
\end{tabular}
}
\end{table*}

# Conclusion and Future Works

To solve the performance and setting problems of Python and its libraries, we have proposed to convert an Elixir code using Flow to a GPU executable code because such an Elixir code fits the SIMD architecture that is adopted as GPUs.

We have demonstrated the effectiveness of our Hastega method by the Logistic Maps application. We have implemented the native code in our benchmark in Rust using Rustler and ocl. This leads to awesome easiness of setting. Our implementation is published in GitHub \cite{logistic_map}.

We have conducted the performance evaluation of the experimental implementation of GPGPU by Elixir and Rustler. We have got the following results:

* Elixir and Rustler code using GPU is 4.43--8.23 times and 7.43--9.64 times faster than pure Elixir and Python code executed by only CPU, respectively.
* The performance gap of Elixir and Rustler code using GPU by our strategy is only 1.48--1.54 times compared with native code using GPU.
* Additionally, our Hastega method for Elixir achieves 3.67 times faster than Python code with GPU.

We realize that Erlang VM is not enough performance for such optimization, including elimination of the conversions between lists and vectors, which is the main reason of the overhead. Thus, We will implement Hastega, which is a new processing system of Elixir, which has a enough ability to drive GPUs and optimize the conversions.

We also have a plan to implement mathematic and machine learning libraries in Elixir, which is implemented using Flow. Of course, we will apply Hastega to the libraries, and compare them with that of Python.
